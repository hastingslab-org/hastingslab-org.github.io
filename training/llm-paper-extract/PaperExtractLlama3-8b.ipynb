{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f70cb4-4eb9-4bbd-b8a2-18bf9ef3562c",
   "metadata": {
    "id": "36f70cb4-4eb9-4bbd-b8a2-18bf9ef3562c"
   },
   "source": [
    "# Extracting information from paper\n",
    "\n",
    "This notebook illustrates some examples of working with text data using small, local language models.\n",
    "\n",
    "## Running this notebook on a newer MacBook with Apple Silicon Chip\n",
    "\n",
    "You will need an environment with Python and Jupyter installed. To create an environment with Anaconda for Python 3.12, execute: \n",
    "\n",
    "```\n",
    "conda create --name llm-narrative python=3.12\n",
    "conda activate llm-narrative\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "## Running this notebook on older MacBooks or any other machine\n",
    "\n",
    "Please run this script on [Google Colab](https://colab.research.google.com/). After opening the notebook there, please change the settings to using a GPU, check [here](https://www.geeksforgeeks.org/how-to-use-gpu-in-google-colab/) for instructions on how to do that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3c0d5-b1f6-4937-9cfa-4d74de9fc7d2",
   "metadata": {
    "id": "f0b3c0d5-b1f6-4937-9cfa-4d74de9fc7d2"
   },
   "source": [
    "### Install required libraries\n",
    "\n",
    "For the newer MacBooks with Apple Chips we will use `mlx-lm` to load a small, quantized version of the Llama 3 8b instruct model, so that it can run on a single laptop (https://ollama.com/library/llama3). For older MacBooks and other machines we will use a quantized version of the model provided by the hugging face community (https://huggingface.co/astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit).\n",
    "\n",
    "Depending on the machine, different packages are required and will be installed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e769d5-d3c8-405c-8a05-e5dd1ec7b23c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2e769d5-d3c8-405c-8a05-e5dd1ec7b23c",
    "outputId": "bdbaad62-1046-41f0-d7e9-2737146ef73c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlx-lm in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (0.14.3)\n",
      "Requirement already satisfied: torch in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: transformers in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (4.41.2)\n",
      "Requirement already satisfied: mlx>=0.14.1 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from mlx-lm) (0.15.1)\n",
      "Requirement already satisfied: numpy in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from mlx-lm) (2.0.0)\n",
      "Requirement already satisfied: protobuf in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from mlx-lm) (5.27.1)\n",
      "Requirement already satisfied: pyyaml in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from mlx-lm) (6.0.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from mlx-lm) (3.1.4)\n",
      "Requirement already satisfied: filelock in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: fsspec in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from jinja2->mlx-lm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import requests\n",
    "\n",
    "# for the newer MacBooks with the Apple Chip\n",
    "# changed for testing but change back later\n",
    "if platform.processor() == 'arm':\n",
    "    ! pip install mlx-lm torch transformers\n",
    "# for all other machines\n",
    "else:\n",
    "    ! pip install torch transformers optimum accelerate auto-gptq bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac10bee-5c79-472d-a39a-7ff9326cdd0f",
   "metadata": {
    "id": "fac10bee-5c79-472d-a39a-7ff9326cdd0f"
   },
   "source": [
    "### Install Llama 3 - 8b\n",
    "Next we install the quantized version of the Llama 8b language model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bac8054-5e69-432b-9b8e-9372b69084ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "1bac8054-5e69-432b-9b8e-9372b69084ac",
    "outputId": "5b40de9a-b209-4cc3-f203-edfc42f8f35f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff499e169944baa924940f009203b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "if platform.processor() == 'arm':\n",
    "    from mlx_lm import load, generate\n",
    "    model, tokenizer = load(\"mlx-community/Meta-Llama-3-8B-Instruct-4bit\")\n",
    "else:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "    import torch\n",
    "\n",
    "    MODEL_ID=\"astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit\")\n",
    "\n",
    "    config = AutoConfig.from_pretrained(MODEL_ID)\n",
    "    config.quantization_config[\"disable_exllama\"] = False\n",
    "    config.quantization_config[\"exllama_config\"] = {\"version\":2}\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID, \n",
    "            device_map='auto', \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            trust_remote_code=True, \n",
    "            # low_cpu_mem_usage=True,\n",
    "            # load_in_4bit=True,\n",
    "            config=config,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b151146-c274-458d-a844-343aaf7977d5",
   "metadata": {
    "id": "1b151146-c274-458d-a844-343aaf7977d5"
   },
   "source": [
    "### Running the model with an example prompt\n",
    "\n",
    "We show that the model can run with an example prompt. First we define the system prompt, which tells the model what character to adopt. Then we give it an instruction to introduce itself. Again, depending on the machine and therefore model used, we use slightly different functions to generate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c40af130-0e32-4d28-9808-94d8e7345c17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "c40af130-0e32-4d28-9808-94d8e7345c17",
    "outputId": "1948ffb3-27e8-42e7-858c-f8f109f90e6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! My name is Ada, and I'm a helpful chatbot assistant. I'm here to assist you with any questions or tasks you may have. I'm a large language model, trained on a vast amount of text data, which enables me to understand and respond to natural language inputs.\\n\\nI'm designed to be friendly, approachable, and knowledgeable. I can help you with a wide range of topics, from general knowledge and entertainment to more specific areas like science, technology, and health....\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from IPython.display import display\n",
    "\n",
    "SYSTEM_MSG = \"You are a helpful chatbot assistant.\"\n",
    "\n",
    "def generateFromPrompt(promptStr,maxTokens=100):\n",
    "    if platform.processor() == 'arm':\n",
    "      messages = [ {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "              {\"role\": \"user\", \"content\": promptStr}, ]\n",
    "      input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "      prompt = tokenizer.decode(input_ids)\n",
    "      response = generate(model, tokenizer, prompt=prompt,max_tokens=maxTokens)\n",
    "    else:\n",
    "      message = [{\"role\": \"user\", \"content\": promptStr},]\n",
    "      pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,max_new_tokens=maxTokens)\n",
    "      result = pipe(message)\n",
    "      response = result[0]['generated_text'][1]['content']\n",
    "    return(response)\n",
    "\n",
    "\n",
    "response = generateFromPrompt(\"Please introduce yourself\")\n",
    "\n",
    "print(response+\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929ce85-9df6-48be-a374-1bd6f2bb50e8",
   "metadata": {},
   "source": [
    "###  Now we need the following functions to search the internet for papers. We will use the API OpenAlex for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333800fd-f288-4f9a-a178-449896a02b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse the text \n",
    "def reconstruct_text(inverted_index):\n",
    "    word_index = [] \n",
    "    for k,v in inverted_index.items(): \n",
    "        for index in v: \n",
    "            word_index.append([k,index])\n",
    "            \n",
    "    word_index = sorted(word_index,key = lambda x : x[1])\n",
    "    \n",
    "    word_list = []\n",
    "    for i in range(len(word_index)):\n",
    "        word_list.append(word_index[i][0])\n",
    "        \n",
    " \n",
    "    separator = ' ' \n",
    "    reconstructed_text = separator.join(word_list) \n",
    "\n",
    "    return reconstructed_text\n",
    "\n",
    "# function that uses openalex to search web for papers\n",
    "def search_openalex(search_phrase, result_count=10, min_year='2013'):\n",
    "    base_url = \"https://api.openalex.org/works\"  # Replace with the actual API endpoint\n",
    "    \n",
    "    # Create filters\n",
    "    filters = [\n",
    "        \"has_abstract:true\",\n",
    "        \"has_fulltext:true\",\n",
    "        f\"from_publication_date:{min_year}-01-01\"\n",
    "    ]\n",
    "    \n",
    "    # Construct the query parameters\n",
    "    params = {\n",
    "    \"search\": search_phrase,\n",
    "    \"filter\": str.join(\",\", filters),  # Only return works with abstracts\n",
    "    \"per_page\": result_count,  # Limit the search\n",
    "    }\n",
    "    \n",
    "    r = requests.get(base_url, params=params)\n",
    "    res_json = r.json()\n",
    "    \n",
    "    abstract_list = []\n",
    "    for i in range(len(res_json[\"results\"])):\n",
    "        abstract_list.append(reconstruct_text(res_json[\"results\"][i]['abstract_inverted_index']))\n",
    "        \n",
    "    return res_json[\"results\"], abstract_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03bcfcb-2a23-4e9f-a23d-da86616aebe5",
   "metadata": {},
   "source": [
    "### Let's try getting papers for our search now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70143972-d14a-49a0-a0b1-6c792caa206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your search phrase\n",
    "search_phrase = 'depression randomized control trial'\n",
    "# enter how many abstracts you want to receive\n",
    "number_of_abstracts = 10\n",
    "# collect the abstracts\n",
    "res_, abstract = search_openalex(search_phrase, number_of_abstracts)\n",
    "# show title\n",
    "res_[0]['title']\n",
    "# show abstract\n",
    "abstract[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c7f22-8b1c-4ad8-b0e6-3e885521ca72",
   "metadata": {},
   "source": [
    "### Now, we want to analyse them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053e28a-1a26-47b9-b3ca-97a64ac8b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract knowledge graphs from paper/ abstract given via input\n",
    "# \n",
    "# Written 2024 by Joshua Sammet, Chair of medical Knowledge and Decision, University of St. Gallen\n",
    "#\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Needed for OpenAlex\n",
    "import requests\n",
    "\n",
    "#logger = get_logger(__name__, log_level=\"INFO\")\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    \n",
    "# Function to reconstruct the inverted index storing of the abstracts\n",
    "def reconstruct_text(inverted_index):\n",
    "    word_index = [] \n",
    "    for k,v in inverted_index.items(): \n",
    "        for index in v: \n",
    "            word_index.append([k,index]) \n",
    "    word_index = sorted(word_index,key = lambda x : x[1])\n",
    "    \n",
    "    word_list = []\n",
    "    for i in range(len(word_index)):\n",
    "        word_list.append(word_index[i][0])\n",
    "        \n",
    "    separator = ' ' \n",
    "    reconstructed_text = separator.join(word_list) \n",
    "\n",
    "    return reconstructed_text\n",
    "\n",
    "# Function to search OpenAlex database\n",
    "def search_openalex(search_phrase, result_count=10, min_year='2013'):\n",
    "    base_url = \"https://api.openalex.org/works\"  # Replace with the actual API endpoint\n",
    "    \n",
    "    # Create filters\n",
    "    filters = [\n",
    "        \"has_abstract:true\",\n",
    "        \"has_fulltext:true\",\n",
    "        f\"from_publication_date:{min_year}-01-01\"\n",
    "    ]\n",
    "    \n",
    "    # Construct the query parameters\n",
    "    params = {\n",
    "    \"search\": search_phrase,\n",
    "    \"filter\": str.join(\",\", filters),  # Only return works with abstracts\n",
    "    \"per_page\": result_count,  # Limit the search\n",
    "    }\n",
    "    \n",
    "    r = requests.get(base_url, params=params)\n",
    "    res_json = r.json()\n",
    "    \n",
    "    abstract_list = []\n",
    "    for i in range(len(res_json[\"results\"])):\n",
    "        abstract_list.append(reconstruct_text(res_json[\"results\"][i]['abstract_inverted_index']))\n",
    "        \n",
    "    return res_json[\"results\"], abstract_list\n",
    "\n",
    "#--------------Prompts--------------\n",
    "PICO_message_abstract=\"\"\"You are an expert agent specialized in extracting PICO elements on abstracts from scientific publications. \n",
    "The PICO elements are population, intervention, comparison and outcome. Your task is to identify the entities and \n",
    "relations requested from an abstract of an scientific paper that is given to you in a prompt.\n",
    "You must generate the output in a JSON containing a list with JSON objects having the following keys: \n",
    "\"head\", \"head_type\", \"relation\", \"tail\", and \"tail_type\".\n",
    "The \"head\" key must contain the text of the extracted entity from the provided user prompt, \n",
    "the \"head_type\" key must contain the type of the extracted head entity which must be one of the PICO elements, the \"relation\" key must contain the type of relation \n",
    "between the \"head\" and the \"tail\", the \"tail\" key must represent the text of an extracted entity which is the tail\n",
    "of the relation, and the \"tail_type\" key must contain the type of the tail entity. Attempt to extract around 10 entities and relations.\n",
    "\"\"\"\n",
    "PICO_bulletpoints_abstract=\"\"\"You are an expert agent specialized in extracting PICO elements on abstracts from scientific publications. \n",
    "The PICO elements are population, intervention, comparison and outcome. Your task is to identify and extract the content from an abstract of an scientific paper that is given to you in a prompt.\n",
    "You must generate the output in the form of a list of bullet points. The content of each bullet point should summarize an aspect of the abstract with regard to at least one PICO element. Please mention the relevant PICO elements at the beginning of each bullet point.\n",
    "Attempt to extract all relevant content from the abstract with around 10 bullet points.\n",
    "\"\"\"\n",
    "#Answer wih 'I am ready' if you understood.\"\"\"\n",
    "\n",
    "#the \"head_type\" key must contain the type of the extracted head entity which must be a topic from the field of clinical trials in medicine or one of the PICO elements\n",
    "\n",
    "PICO_message_papers=\"\"\"You are an expert agent specialized in extracting PICO elements from scientific publications. \n",
    "The PICO elements are population, intervention, comparison and outcome. They are created to systemically review clinical literature.\n",
    "Your task is to identify the entities and relations requested from an scientific paper that is given to you in a user prompt.\n",
    "You must generate the output in a JSON containing a list with JSON objects having the following keys: \n",
    "\"head\", \"head_type\", \"relation\", \"tail\", and \"tail_type\".\n",
    "The \"head\" key must contain the text of the extracted entity from the provided user prompt, \n",
    "the \"head_type\" key must contain the type of the extracted head entity which must be a topic from the field of \n",
    "clinical trials in medicine or one of the PICO elements, the \"relation\" key must contain the type of relation \n",
    "between the \"head\" and the \"tail\", the \"tail\" key must represent the text of an extracted entity which is the tail\n",
    "of the relation, and the \"tail_type\" key must contain the type of the tail entity. Attempt to extract as\n",
    "many entities and relations as you can.\n",
    "Answer wih 'I am ready' if you understood.\"\"\"\n",
    "entity_message=\"\"\"\"\"\"\n",
    "entity_and_relations_message=\"\"\"\"\"\"\n",
    "relations_message=\"\"\"\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Function to find publications of interest and analyse them\n",
    "INPUT:\n",
    "model_name - name of specified model that should be used\n",
    "search_phrase - Topic that should be search for (e.g. 'depression treatment random controlled trial'\n",
    "number_of_abstracts - How many papers should be checked\n",
    "entities - define the entities in the knowledge graph\n",
    "relations - define the relations in the knowledge graph\n",
    "prompt - Specify which prompt should be used to instruct the model\n",
    "\n",
    "OUTPUT:\n",
    "returns json file with knowledge graph for each apper\n",
    "\"\"\"\n",
    "def find_and_analyse(model_name, search_phrase, number_of_abstracts=10, entities=None, relations=None, prompt=None):\n",
    "    # Get papers from OpenAlex\n",
    "    results, abstracts = search_openalex(search_phrase, number_of_abstracts)\n",
    "    \n",
    "    # Define system prompt message\n",
    "    if prompt==None:\n",
    "        if entities==None:\n",
    "            if relations==None:\n",
    "                system_message = PICO_bulletpoints_abstract #PICO_message_abstract\n",
    "            else:\n",
    "                system_message = relations_message\n",
    "        else:\n",
    "            if relations==None:\n",
    "                system_message = entity_message\n",
    "            else:\n",
    "                system_message = entity_and_relations_message\n",
    "    else:\n",
    "        system_message = prompt\n",
    "    \n",
    "    # Setup LLM\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map='cuda', attn_implementation=\"flash_attention_2\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_name\n",
    "        # , use_fast=False, # only use for Orca because of bug\n",
    "    )\n",
    "    prompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n <|im_start|> assistant\\n \"\n",
    "\n",
    "    #inputs = tokenizer(prompt, return_tensors='pt', return_attention_mask=False).to(\"cuda\")\n",
    "    #output_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    for i in range(len(abstracts)):\n",
    "        print(f\"For paper number {i+1}, titled {results[i]['title']}, the knowledge graph of the abstract gives the following information:\\n\")\n",
    "        #prompt = f\"<|im_start|>user\\n Extract the knowledge graph fom the following abstract:\\n {abstracts[i]}<|im_end|>\\n<|im_start|> assistant\\n \"\n",
    "        prompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n Extract the knowledge graph fom the following abstract:\\n {abstracts[i]}<|im_end|>\\n<|im_start|> assistant\\n \"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', return_attention_mask=False).to(\"cuda\")\n",
    "        output_ids = model.generate(inputs[\"input_ids\"],max_new_tokens=250)\n",
    "        answer = tokenizer.batch_decode(output_ids)[0]\n",
    "        cut_answer = answer.split(\"<|im_start|> assistant\\n\",1)[1]\n",
    "        #json_answer = json.loads(cut_answer.split(\"</s>\",1)[0])\n",
    "        print(cut_answer + '\\n')\n",
    "        #print(answer)\n",
    "        \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13729b0-82bf-4450-9573-963334920a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the papers\n",
    "automated_paper_analyser.find_and_analyse(\"meta-llama/Meta-Llama-3-8B\", search_phrase, number_of_abstracts)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
