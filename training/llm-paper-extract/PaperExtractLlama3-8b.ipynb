{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f70cb4-4eb9-4bbd-b8a2-18bf9ef3562c",
   "metadata": {
    "id": "36f70cb4-4eb9-4bbd-b8a2-18bf9ef3562c"
   },
   "source": [
    "# Extracting information from paper\n",
    "\n",
    "This notebook illustrates some examples of working with text data using small, local language models.\n",
    "\n",
    "## Running this notebook on a newer MacBook with Apple Silicon Chip\n",
    "\n",
    "You will need an environment with Python and Jupyter installed. To create an environment with Anaconda for Python 3.12, execute: \n",
    "\n",
    "```\n",
    "conda create --name llm-narrative python=3.12\n",
    "conda activate llm-narrative\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "## Running this notebook on older MacBooks or any other machine\n",
    "\n",
    "Please run this script on [Google Colab](https://colab.research.google.com/). After opening the notebook there, please change the settings to using a GPU, check [here](https://www.geeksforgeeks.org/how-to-use-gpu-in-google-colab/) for instructions on how to do that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3c0d5-b1f6-4937-9cfa-4d74de9fc7d2",
   "metadata": {
    "id": "f0b3c0d5-b1f6-4937-9cfa-4d74de9fc7d2"
   },
   "source": [
    "### Install required libraries\n",
    "\n",
    "For the newer MacBooks with Apple Chips we will use `mlx-lm` to load a small, quantized version of the Llama 3 8b instruct model, so that it can run on a single laptop (https://ollama.com/library/llama3). For older MacBooks and other machines we will use a quantized version of the model provided by the hugging face community (https://huggingface.co/astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit).\n",
    "\n",
    "Depending on the machine, different packages are required and will be installed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e769d5-d3c8-405c-8a05-e5dd1ec7b23c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2e769d5-d3c8-405c-8a05-e5dd1ec7b23c",
    "outputId": "bdbaad62-1046-41f0-d7e9-2737146ef73c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlx-lm in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (0.14.3)\n",
      "Requirement already satisfied: torch in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: transformers in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (4.41.2)\n",
      "Requirement already satisfied: mlx>=0.14.1 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from mlx-lm) (0.15.1)\n",
      "Requirement already satisfied: numpy in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from mlx-lm) (2.0.0)\n",
      "Requirement already satisfied: protobuf in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from mlx-lm) (5.27.1)\n",
      "Requirement already satisfied: pyyaml in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from mlx-lm) (6.0.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from mlx-lm) (3.1.4)\n",
      "Requirement already satisfied: filelock in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: fsspec in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from jinja2->mlx-lm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/ctumes/miniforge3/envs/llm-narrative/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "# for the newer MacBooks with the Apple Chip\n",
    "# changed for testing but change back later\n",
    "if platform.processor() == 'arm':\n",
    "    ! pip install mlx-lm torch transformers\n",
    "# for all other machines\n",
    "else:\n",
    "    ! pip install torch transformers optimum accelerate auto-gptq bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac10bee-5c79-472d-a39a-7ff9326cdd0f",
   "metadata": {
    "id": "fac10bee-5c79-472d-a39a-7ff9326cdd0f"
   },
   "source": [
    "### Install Llama 3 - 8b\n",
    "Next we install the quantized version of the Llama 8b language model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bac8054-5e69-432b-9b8e-9372b69084ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "1bac8054-5e69-432b-9b8e-9372b69084ac",
    "outputId": "5b40de9a-b209-4cc3-f203-edfc42f8f35f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff499e169944baa924940f009203b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "if platform.processor() == 'arm':\n",
    "    from mlx_lm import load, generate\n",
    "    model, tokenizer = load(\"mlx-community/Meta-Llama-3-8B-Instruct-4bit\")\n",
    "else:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "    import torch\n",
    "\n",
    "    MODEL_ID=\"astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit\")\n",
    "\n",
    "    config = AutoConfig.from_pretrained(MODEL_ID)\n",
    "    config.quantization_config[\"disable_exllama\"] = False\n",
    "    config.quantization_config[\"exllama_config\"] = {\"version\":2}\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID, \n",
    "            device_map='auto', \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            trust_remote_code=True, \n",
    "            # low_cpu_mem_usage=True,\n",
    "            # load_in_4bit=True,\n",
    "            config=config,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b151146-c274-458d-a844-343aaf7977d5",
   "metadata": {
    "id": "1b151146-c274-458d-a844-343aaf7977d5"
   },
   "source": [
    "### Running the model with an example prompt\n",
    "\n",
    "We show that the model can run with an example prompt. First we define the system prompt, which tells the model what character to adopt. Then we give it an instruction to introduce itself. Again, depending on the machine and therefore model used, we use slightly different functions to generate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c40af130-0e32-4d28-9808-94d8e7345c17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "c40af130-0e32-4d28-9808-94d8e7345c17",
    "outputId": "1948ffb3-27e8-42e7-858c-f8f109f90e6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! My name is Ada, and I'm a helpful chatbot assistant. I'm here to assist you with any questions or tasks you may have. I'm a large language model, trained on a vast amount of text data, which enables me to understand and respond to natural language inputs.\\n\\nI'm designed to be friendly, approachable, and knowledgeable. I can help you with a wide range of topics, from general knowledge and entertainment to more specific areas like science, technology, and health....\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from IPython.display import display\n",
    "\n",
    "SYSTEM_MSG = \"You are a helpful chatbot assistant.\"\n",
    "\n",
    "def generateFromPrompt(promptStr,maxTokens=100):\n",
    "    if platform.processor() == 'arm':\n",
    "      messages = [ {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "              {\"role\": \"user\", \"content\": promptStr}, ]\n",
    "      input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "      prompt = tokenizer.decode(input_ids)\n",
    "      response = generate(model, tokenizer, prompt=prompt,max_tokens=maxTokens)\n",
    "    else:\n",
    "      message = [{\"role\": \"user\", \"content\": promptStr},]\n",
    "      pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,max_new_tokens=maxTokens)\n",
    "      result = pipe(message)\n",
    "      response = result[0]['generated_text'][1]['content']\n",
    "    return(response)\n",
    "\n",
    "\n",
    "response = generateFromPrompt(\"Please introduce yourself\")\n",
    "\n",
    "print(response+\"...\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
