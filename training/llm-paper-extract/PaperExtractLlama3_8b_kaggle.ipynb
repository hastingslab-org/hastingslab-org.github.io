{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"011204a348c9404a91310bcd7825758b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b14e59627ffb4a2b83bbbbebb045ed5e","IPY_MODEL_5971dadeb2f94b6c891eff78979433fb","IPY_MODEL_ab5c139bb01742288b91cc22378eac80"],"layout":"IPY_MODEL_02fb343a8adf472ab16964d10ff11809"}},"b14e59627ffb4a2b83bbbbebb045ed5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_990ac11e58a545f7ac73e9d640152491","placeholder":"​","style":"IPY_MODEL_b54b1043fc4844bfa9fb2492773aabc2","value":"model.safetensors: 100%"}},"5971dadeb2f94b6c891eff78979433fb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea931cfae8374fcd947c866baac850c0","max":5735720616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a726f6e083c949a9a4760a12dd458c0d","value":5735720616}},"ab5c139bb01742288b91cc22378eac80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ce40a1ec88448ea862f99e43bee5314","placeholder":"​","style":"IPY_MODEL_b1b6842d394a47f98bf5c22eb317f15f","value":" 5.74G/5.74G [00:37&lt;00:00, 108MB/s]"}},"02fb343a8adf472ab16964d10ff11809":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"990ac11e58a545f7ac73e9d640152491":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b54b1043fc4844bfa9fb2492773aabc2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea931cfae8374fcd947c866baac850c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a726f6e083c949a9a4760a12dd458c0d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ce40a1ec88448ea862f99e43bee5314":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1b6842d394a47f98bf5c22eb317f15f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"421dda6dbcf2455d8d0fe1b4fb094a93":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a4aabb58311945e4946f23a574a25484","IPY_MODEL_ed326ac1c43f40a1b109ef6bf934de8a","IPY_MODEL_20a3c09e9e8d4bbca1fc5b1a51d23be0"],"layout":"IPY_MODEL_d57150fd23c5485a9f74158a8fd654d5"}},"a4aabb58311945e4946f23a574a25484":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab19e1e356184f72a9d594082e13bf25","placeholder":"​","style":"IPY_MODEL_b86506991f9340639036480ccebb5748","value":"generation_config.json: 100%"}},"ed326ac1c43f40a1b109ef6bf934de8a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbcc7bbf23254f1a8d9780351146425d","max":136,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff4e44aabd8f4ce396dabd77264d1718","value":136}},"20a3c09e9e8d4bbca1fc5b1a51d23be0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ebeba9229094db7a6b0f828da76c7e0","placeholder":"​","style":"IPY_MODEL_426a3b890c084678a98283bfdf7ae619","value":" 136/136 [00:00&lt;00:00, 7.56kB/s]"}},"d57150fd23c5485a9f74158a8fd654d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab19e1e356184f72a9d594082e13bf25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b86506991f9340639036480ccebb5748":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bbcc7bbf23254f1a8d9780351146425d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff4e44aabd8f4ce396dabd77264d1718":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ebeba9229094db7a6b0f828da76c7e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"426a3b890c084678a98283bfdf7ae619":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Extracting information from paper\n\nThis notebook illustrates some examples of working with text data using small, local language models.\n\n## Running this notebook on a newer MacBook with Apple Silicon Chip\n\nYou will need an environment with Python and Jupyter installed. To create an environment with Anaconda for Python 3.12, execute:\n\n```\nconda create --name llm-narrative python=3.12\nconda activate llm-narrative\nconda install jupyter\njupyter notebook\n```\n\n## Running this notebook on older MacBooks or any other machine\n\nPlease run this script on [Google Colab](https://colab.research.google.com/). After opening the notebook there, please change the settings to using a GPU, check [here](https://www.geeksforgeeks.org/how-to-use-gpu-in-google-colab/) for instructions on how to do that.\n\n","metadata":{"id":"36f70cb4-4eb9-4bbd-b8a2-18bf9ef3562c"}},{"cell_type":"markdown","source":"## Install required libraries\n\nFor the newer MacBooks with Apple Chips we will use `mlx-lm` to load a small, quantized version of the Llama 3 8b instruct model, so that it can run on a single laptop (https://ollama.com/library/llama3). For older MacBooks and other machines we will use a quantized version of the model provided by the hugging face community (https://huggingface.co/astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit).\n\nDepending on the machine, different packages are required and will be installed below.","metadata":{"id":"f0b3c0d5-b1f6-4937-9cfa-4d74de9fc7d2"}},{"cell_type":"code","source":"import platform\nimport requests\n\n# For the paper analyser\nimport torch\nimport transformers\nimport argparse\nimport logging\nimport json\nimport os\nimport accelerate\n\n","metadata":{"id":"e2e769d5-d3c8-405c-8a05-e5dd1ec7b23c","scrolled":true,"execution":{"iopub.status.busy":"2024-07-04T20:45:01.583209Z","iopub.execute_input":"2024-07-04T20:45:01.583480Z","iopub.status.idle":"2024-07-04T20:45:04.445717Z","shell.execute_reply.started":"2024-07-04T20:45:01.583450Z","shell.execute_reply":"2024-07-04T20:45:04.444896Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# for the newer MacBooks with the Apple Chip\n# changed for testing but change back later\n! pip install torch transformers optimum accelerate auto-gptq bitsandbytes","metadata":{"id":"_osyZoSZIknd","execution":{"iopub.status.busy":"2024-07-04T20:44:48.791419Z","iopub.execute_input":"2024-07-04T20:44:48.791807Z","iopub.status.idle":"2024-07-04T20:45:01.577184Z","shell.execute_reply.started":"2024-07-04T20:44:48.791768Z","shell.execute_reply":"2024-07-04T20:45:01.576086Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: optimum in /opt/conda/lib/python3.10/site-packages (1.21.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: auto-gptq in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from optimum) (15.0.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (2.19.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\nRequirement already satisfied: rouge in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.0.1)\nRequirement already satisfied: gekko in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.2.1)\nRequirement already satisfied: peft>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.11.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (3.20.3)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.4)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Install Llama 3 - 8b\nNext we install the quantized version of the Llama 8b language model.\n\n","metadata":{"id":"fac10bee-5c79-472d-a39a-7ff9326cdd0f"}},{"cell_type":"code","source":"if platform.processor() == 'arm':\n    from mlx_lm import load, generate\n    model, tokenizer = load(\"mlx-community/Meta-Llama-3-8B-Instruct-4bit\")\nelse:\n    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n    import torch\n\n    MODEL_ID=\"astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit\"\n    tokenizer = AutoTokenizer.from_pretrained(\"astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit\")\n\n    config = AutoConfig.from_pretrained(MODEL_ID)\n    config.quantization_config[\"disable_exllama\"] = False\n    config.quantization_config[\"exllama_config\"] = {\"version\":2}\n\n    model = AutoModelForCausalLM.from_pretrained(\n            MODEL_ID,\n            device_map='auto',\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n            # low_cpu_mem_usage=True,\n            # load_in_4bit=True,\n            config=config,\n        )","metadata":{"id":"1bac8054-5e69-432b-9b8e-9372b69084ac","outputId":"f73b53b0-664f-4c4a-9078-79f6e7b4de67","scrolled":true,"execution":{"iopub.status.busy":"2024-07-04T20:45:04.446811Z","iopub.execute_input":"2024-07-04T20:45:04.447193Z","iopub.status.idle":"2024-07-04T20:45:46.323989Z","shell.execute_reply.started":"2024-07-04T20:45:04.447168Z","shell.execute_reply":"2024-07-04T20:45:46.323019Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nUsing `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n2024-07-04 20:45:05.716699: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-04 20:45:05.716763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-04 20:45:05.718136: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cd524f3be2b4a58af9071e1b2e65ce4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4481: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n  warnings.warn(\nSome weights of the model checkpoint at astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8decf017ff4140b39ca32245afedbfc4"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Running the model with an example prompt\n\nWe show that the model can run with an example prompt. First we define the system prompt, which tells the model what character to adopt. Then we give it an instruction to introduce itself. Again, depending on the machine and therefore model used, we use slightly different functions to generate output.","metadata":{"id":"1b151146-c274-458d-a844-343aaf7977d5"}},{"cell_type":"code","source":"# neccessary because Kaggles GPUs are weird'\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:46:08.502102Z","iopub.execute_input":"2024-07-04T20:46:08.502484Z","iopub.status.idle":"2024-07-04T20:46:08.506952Z","shell.execute_reply.started":"2024-07-04T20:46:08.502454Z","shell.execute_reply":"2024-07-04T20:46:08.505998Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nfrom IPython.display import display\n\nSYSTEM_MSG = \"You are a helpful chatbot assistant.\"\n\ndef generateFromPrompt(promptStr,maxTokens=100):\n    if platform.processor() == 'arm':\n      messages = [ {\"role\": \"system\", \"content\": SYSTEM_MSG},\n              {\"role\": \"user\", \"content\": promptStr}, ]\n      input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n      prompt = tokenizer.decode(input_ids)\n      response = generate(model, tokenizer, prompt=prompt,max_tokens=maxTokens)\n    else:\n      message = [{\"role\": \"user\", \"content\": promptStr},]\n      pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,max_new_tokens=maxTokens)\n      result = pipe(message)\n      response = result[0]['generated_text'][1]['content']\n    return(response)\n\n\nresponse = generateFromPrompt(\"Please introduce yourself\")\n\nprint(response+\"...\")","metadata":{"id":"c40af130-0e32-4d28-9808-94d8e7345c17","outputId":"28c4c1a3-dbf1-4cc8-8f0f-15f8d73f6fd8","execution":{"iopub.status.busy":"2024-07-04T20:46:10.560223Z","iopub.execute_input":"2024-07-04T20:46:10.560957Z","iopub.status.idle":"2024-07-04T20:47:46.643861Z","shell.execute_reply.started":"2024-07-04T20:46:10.560921Z","shell.execute_reply":"2024-07-04T20:47:46.642807Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Nice to meet you! I'm LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm trained on a massive dataset of text from the internet and can generate human-like responses to a wide range of topics and questions. I'm here to help answer your questions, provide information, and even have a fun conversation with you! What would you like to talk about?...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"###  Now we need the following functions to search the internet for papers. We will use the API OpenAlex for this.","metadata":{"id":"1929ce85-9df6-48be-a374-1bd6f2bb50e8"}},{"cell_type":"markdown","source":"First, we define two functions that we need to porperly use OpenAlex.<br>\n`reconstruct_text` is used to extract the abstract.<br>\n`search_openalex` searches OpenAlex for a given search phrase and returns a specified number of found articles.","metadata":{"id":"e90e2e34"}},{"cell_type":"code","source":"# function to parse the text\ndef reconstruct_text(inverted_index):\n    word_index = []\n    for k,v in inverted_index.items():\n        for index in v:\n            word_index.append([k,index])\n\n    word_index = sorted(word_index,key = lambda x : x[1])\n\n    word_list = []\n    for i in range(len(word_index)):\n        word_list.append(word_index[i][0])\n\n    separator = ' '\n    reconstructed_text = separator.join(word_list)\n\n    return reconstructed_text\n\n# function that uses openalex to search web for papers\ndef search_openalex(search_phrase, result_count=10, min_year='2013'):\n    base_url = \"https://api.openalex.org/works\"  # Replace with the actual API endpoint\n\n    # Create filters\n    filters = [\n        \"has_abstract:true\",\n        \"has_fulltext:true\",\n        f\"from_publication_date:{min_year}-01-01\"\n    ]\n\n    # Construct the query parameters\n    params = {\n    \"search\": search_phrase,\n    \"filter\": str.join(\",\", filters),  # Only return works with abstracts\n    \"per_page\": result_count,  # Limit the search\n    }\n\n    r = requests.get(base_url, params=params)\n    res_json = r.json()\n\n    abstract_list = []\n    for i in range(len(res_json[\"results\"])):\n        abstract_list.append(reconstruct_text(res_json[\"results\"][i]['abstract_inverted_index']))\n\n    return res_json[\"results\"], abstract_list","metadata":{"id":"333800fd-f288-4f9a-a178-449896a02b17","execution":{"iopub.status.busy":"2024-07-04T20:48:40.359662Z","iopub.execute_input":"2024-07-04T20:48:40.360143Z","iopub.status.idle":"2024-07-04T20:48:40.369577Z","shell.execute_reply.started":"2024-07-04T20:48:40.360100Z","shell.execute_reply":"2024-07-04T20:48:40.368628Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Let's try getting papers for our search now!","metadata":{"id":"d03bcfcb-2a23-4e9f-a23d-da86616aebe5"}},{"cell_type":"code","source":"# enter your search phrase\nsearch_phrase = 'depression randomized control trial'\n# enter how many abstracts you want to receive\nnumber_of_abstracts = 10\n# collect the abstracts\nres_, abstract = search_openalex(search_phrase, number_of_abstracts)\n# show title\nprint(res_[0]['title'])\n# show abstract\nabstract[0]","metadata":{"id":"70143972-d14a-49a0-a0b1-6c792caa206b","outputId":"5267b24a-c830-428b-be35-03a81fea20e8","execution":{"iopub.status.busy":"2024-07-04T20:48:41.819029Z","iopub.execute_input":"2024-07-04T20:48:41.819406Z","iopub.status.idle":"2024-07-04T20:48:42.341558Z","shell.execute_reply.started":"2024-07-04T20:48:41.819373Z","shell.execute_reply":"2024-07-04T20:48:42.340586Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Rethinking the Dose-Response Relationship Between Usage and Outcome in an Online Intervention for Depression: Randomized Controlled Trial\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'There is now substantial evidence that Web-based interventions can be effective at changing behavior and successfully treating psychological disorders. However, interest in the impact of usage on intervention outcomes has only been developed recently. To date, persistence with or completion of the intervention has been the most commonly reported metric of use, but this does not adequately describe user behavior online. Analysis of alternative measures of usage and their relationship to outcome may help to understand how much of the intervention users may need to obtain a clinically significant benefit from the program.The objective of this study was to determine which usage metrics, if any, are associated with outcome in an online depression treatment trial.Cardiovascular Risk E-couch Depression Outcome (CREDO) is a randomized controlled trial evaluating an unguided Web-based program (E-couch) based on cognitive behavioral therapy and interpersonal therapy for people with depression and cardiovascular disease. In all, 280 participants in the active arm of the trial commenced the program, delivered in 12 modules containing pages of text and activities. Usage data (eg, number of log-ins, modules completed, time spent online, and activities completed) were captured automatically by the program interface. We estimated the association of these and composite metrics with the outcome of a clinically significant improvement in depression score on the Patient Health Questionnaire (PHQ-9) of ≥ 5 points.In all, 214/280 (76.4%) participants provided outcome data at the end of the 12-week period and were included in the analysis. Of these, 94 (43.9%) participants obtained clinically significant improvement. Participants logged into the program an average of 18.7 times (SD 8.3) with most (62.1%, 133/214) completing all 12 modules. Average time spent online per log-in was 17.3 minutes (SD 10.5). Participants completed an average of 9 of 18 activities available within the program. In a multivariate regression model, only the number of activities completed per log-in was associated with a clinically significant outcome (OR 2.82, 95% CI 1.05-7.59). The final model predicted 7.4% of variance in outcome. Curve estimates indicated that significant logarithmic (P=.009) and linear (P=.002) relationships existed between activities completed per log-in and clinically significant change.Only one objective measure of usage was independently associated with better outcome of a Web-based intervention of known effectiveness. The 4 usage metrics retained in the final step of the regression accounted for little outcome variance. Medium level users appeared to have little additional benefit compared to low users indicating that assumptions of a linear relationship between use and outcome may be too simplistic and further models and variables need to be explored to adequately understand the relationship.'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Now, we want to analyse them","metadata":{"id":"790c7f22-8b1c-4ad8-b0e6-3e885521ca72"}},{"cell_type":"markdown","source":"The next section provides you with a function you can call to search OpenALex and get the LLMs summary returned. <br>\nIt comes with default settings, so you only have to give a search phrase. However, we can refine and customize it!","metadata":{"id":"b1d9ffc1"}},{"cell_type":"code","source":"# Function to extract knowledge graphs from paper/ abstract given via input\n#\n# Written 2024 by Joshua Sammet, Chair of medical Knowledge and Decision, University of St. Gallen\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n\n#--------------Different Prompts--------------\nPICO_message_abstract=\"\"\"You are an expert agent specialized in extracting PICO elements on abstracts from scientific publications.\nThe PICO elements are population, intervention, comparison and outcome. Your task is to identify the entities and\nrelations requested from an abstract of an scientific paper that is given to you in a prompt.\nYou must generate the output in a JSON containing a list with JSON objects having the following keys:\n\"head\", \"head_type\", \"relation\", \"tail\", and \"tail_type\".\nThe \"head\" key must contain the text of the extracted entity from the provided user prompt,\nthe \"head_type\" key must contain the type of the extracted head entity which must be one of the PICO elements, the \"relation\" key must contain the type of relation\nbetween the \"head\" and the \"tail\", the \"tail\" key must represent the text of an extracted entity which is the tail\nof the relation, and the \"tail_type\" key must contain the type of the tail entity. Attempt to extract around 10 entities and relations.\n\"\"\"\nPICO_bulletpoints_abstract=\"\"\"You are an expert agent specialized in extracting PICO elements on abstracts from scientific publications.\nThe PICO elements are population, intervention, comparison and outcome. Your task is to identify and extract the content from an abstract of an scientific paper that is given to you in a prompt.\nYou must generate the output in the form of a list of bullet points. The content of each bullet point should summarize an aspect of the abstract with regard to one PICO element. Please mention the relevant PICO elements at the beginning of each bullet point.\n\"\"\"\n\n\"\"\"\nFunction to find publications of interest and analyse them\nINPUT:\nmodel_name - name of specified model that should be used\nsearch_phrase - Topic that should be search for (e.g. 'depression treatment random controlled trial'\nnumber_of_abstracts - How many papers should be checked\nentities - define the entities in the knowledge graph\nrelations - define the relations in the knowledge graph\nprompt - Specify which prompt should be used to instruct the model\n\nOUTPUT:\nreturns json file with knowledge graph for each apper\n\"\"\"\n# This version uses the llama3 model that was already loaded beforehand\ndef find_and_analyse_llama3(search_phrase, number_of_abstracts=1, prompt=None, input_text=None):\n    # Get papers from OpenAlex\n    results, abstracts = search_openalex(search_phrase, number_of_abstracts)\n\n    # Define system prompt message\n    if prompt==None:\n        system_message = PICO_bulletpoints_abstract\n    else:\n        system_message = prompt\n\n    prompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n <|im_start|> assistant\\n \"\n    inputs = tokenizer(prompt, return_tensors='pt', return_attention_mask=False).to(\"cuda\")\n    output_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n\n    if input_text == None:\n        for i in range(len(abstracts)):\n            print(f\"For paper number {i+1}, titled {results[i]['title']}, the abstract gives the following information:\\n\")\n            prompt = f\"<|im_start|>user\\n Extract the PICO elements fom the following abstract:\\n {abstracts[i]}<|im_end|>\\n<|im_start|> assistant\\n \"\n            #prompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n Extract the knowledge graph fom the following abstract:\\n {abstracts[i]}<|im_end|>\\n<|im_start|> assistant\\n \"\n\n            inputs = tokenizer(prompt, return_tensors='pt', return_attention_mask=False).to(\"cuda\")\n            output_ids = model.generate(inputs[\"input_ids\"],max_new_tokens=100)\n            answer = tokenizer.batch_decode(output_ids)[0]\n            cut_answer = answer.split(\"<|im_start|> assistant\\n\",1)[1]\n            print(cut_answer + '\\n')\n    else:\n        print(f\"For the given text, the abstract gives the following information:\\n\")\n        prompt = f\"<|im_start|>user\\n Extract the PICO elements fom the following abstract:\\n {abstracts[i]}<|im_end|>\\n<|im_start|> assistant\\n \"\n        #prompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n Extract the knowledge graph fom the following abstract:\\n {input_text}<|im_end|>\\n<|im_start|> assistant\\n \"\n\n        inputs = tokenizer(prompt, return_tensors='pt', return_attention_mask=False).to(\"cuda\")\n        output_ids = model.generate(inputs[\"input_ids\"],max_new_tokens=100)\n        answer = tokenizer.batch_decode(output_ids)[0]\n        cut_answer = answer.split(\"<|im_start|> assistant\\n\",1)[1]\n        print(cut_answer + '\\n')\n\n\n    return None","metadata":{"id":"0053e28a-1a26-47b9-b3ca-97a64ac8b9d5","execution":{"iopub.status.busy":"2024-07-04T20:48:49.075850Z","iopub.execute_input":"2024-07-04T20:48:49.076295Z","iopub.status.idle":"2024-07-04T20:48:49.090293Z","shell.execute_reply.started":"2024-07-04T20:48:49.076264Z","shell.execute_reply":"2024-07-04T20:48:49.089327Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# enter your search phrase\nsearch_phrase = 'depression randomized control trial'\n# analyse the paper\nfind_and_analyse_llama3(search_phrase)","metadata":{"id":"b13729b0-82bf-4450-9573-963334920a4d","outputId":"03b7deb4-af96-4af6-9b40-01f028a08c83","execution":{"iopub.status.busy":"2024-07-04T20:48:53.059349Z","iopub.execute_input":"2024-07-04T20:48:53.060017Z","iopub.status.idle":"2024-07-04T20:51:49.764689Z","shell.execute_reply.started":"2024-07-04T20:48:53.059983Z","shell.execute_reply":"2024-07-04T20:51:49.763767Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"For paper number 1, titled Rethinking the Dose-Response Relationship Between Usage and Outcome in an Online Intervention for Depression: Randomized Controlled Trial, the abstract gives the following information:\n\n  * PICO elements:\n    * P: Problem/Population: People with depression and cardiovascular disease\n    * I: Intervention: Un-guided Web-based program (E-couch) based on cognitive behavioral therapy and interpersonal therapy\n    * C: Comparison: Not applicable (randomized controlled trial)\n    * O: Outcome: Clinically significant improvement in depression score on the Patient Health Questionnaire (PHQ-9) of ≥ 5 points\n    * E: Exposure: Usage metrics (\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Now it is your turn!\n\nPlay around with the model. You can tackle different questions, depending on your interests and skills:\n- What effect has a change in the search phrase?\n- What effect has a change in the prompt? Can we shift the models focus? How can we make the summaries briefer or more detailed?\n- Test your own inputs! Is the models performance satisfactory? What should be changed to make it better?","metadata":{"id":"4ace95b7"}},{"cell_type":"code","source":"# enter your search phrase\nsearch_phrase = 'depression randomized control trial'\n# enter your prompt\nown_prompt = None\n# enter your input text\nown_abstract = None\n# analyse the paper\nfind_and_analyse_llama3(search_phrase, prompt=own_prompt, input_text=own_abstract)","metadata":{"id":"250ec623"},"execution_count":null,"outputs":[]}]}